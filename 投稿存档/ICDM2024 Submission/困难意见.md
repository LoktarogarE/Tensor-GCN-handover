## tough question aroused by ICDM reviewer



Reviewer1: 

* Q: Can one not get higher order by iteratively doing pairwise combinations? 
* Q: Eq8: ignores the $S_{ik}$ interaction!?  
  * Eq8: $\mathcal  T_{3_{ijk}} = S_{ij}S_{kj}$ 

Reviewer3:

* Q: More explanation is needed about why high-dimensional data is a problem for graph representation learning. Since some theoretical work [1] has talked about the relationship between the dimensionality of the sample size (#nodes) and features, where higher dimensionality comes with better model expressiveness ability, it is necessary for the authors to explain their arguments more.
* Q: The literature review is not comprehensive, some main arguments can be easily rejected. For the approximation ability of different polynomials, how is the sentence "Increasing the order of the polynomials filter directly may even inadvertently amplify this error, since an L2 term was added earlier" supported by the ChebyNetII work? Because different polynomial bases have very different statistical properties.
* Q: The proposal is limited to homophilic graphs. 



Reviewer4:

* Q: I particularly would like to know how much value does the 3-tensor add. For example, how would the experimental results look like if we didn't have the 3-tensor, only the Laplacian? How does this compare to graph diffusion methods, in the most controlled way possible? The simplest ablation I can think of is to remove the tensor component of equation (12) and keep the later part of the network the same. Another variation is to replace the $k=2$ part of the equation (12) with the second-power of the transition matrix to have a direct comparison with DCNNs. 





提取 <!--T_3-->

$X^{l+1}=\sigma((\mathcal{T}_3\times_2X^l\times_3X^l)\times^{(2,3)}_{(1,2)}W)$    



$X^{l+1}=\sigma((\mathcal{T}_4\times_2X^l\times_3X^l\times_4 X^l)\times^{(2,3,4)}_{(1,2,3)}W)$
